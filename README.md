# nlp-examples.
Repository that stores many NLP examples.

-----------------------------------------------------------------------

## Configuration.
The scripts within this repository are configured by a `.env` file, this can be generated by hand using the `.not_an_env` template file inside the [`npl`](https://github.com/tamagochi-descompuesto/nlp-examples/tree/main/nlp) folder which explains all the environment variables that need to be configured. Another way to do this is by running the `make_env.py` file which will make the `.env` file with the default configurations. 

*Note:* Make sure to be localized inside the [`npl`](https://github.com/tamagochi-descompuesto/nlp-examples/tree/main/nlp) folder before you run all the commands correctly, it is highly recommended to use an anaconda or venv environment to run all the scripts in order to have a better experience and performance.

It's important to note that the following variables need to be obtained from the [RapidAPI](https://rapidapi.com/hub) webpage by creating an account and subscribing to the corresponding API service (in this case, the [Deep Translate API](https://rapidapi.com/gatzuma/api/deep-translate1/)):
* `X_RAPID_API_KEY`.
* `X_RAPID_API_HOST_DEEP`.

*Note 2.0:* Each environment variable is explained in the [About environment variables](#about-environment-variables) section or in the [`.not_an_env` file](https://github.com/tamagochi-descompuesto/nlp-examples/blob/main/nlp/.not_an_env).

If you're using an Anaconda environment you will need to download the sqlite3 dll file from the [SQLite download page](https://www.sqlite.org/download.html) and move the `sqlite3.dll` file to your `anaconda/DLLs` folder (check where your local anaconda installation is), then inside the `anaconda/envs` file localize your `DLLs` filder and copy the sqlite3 file too.

Finally, in order for the scripts to work, all the necessary packages need to be installed, run the `pip install -r requirements.txt` to install all the requeriements needed for the scripts, sometimes errors may be raised due to the versioning of the libraries, to fix this you can edit the `requirements.txt` file and eliminate the versions of the corresponding packages.

*Note 3.0:* Each NLP example has a test code if you run each code directly, these are found on the [src](https://github.com/tamagochi-descompuesto/nlp-examples/tree/main/nlp/src) folder.

## Running the scripts.
To test each script just run the `python run.py` file, this will execute every NLP example done in this repository.

## About the NLP examples.
This repository contains 3 real life examples of NLP application, this are:
### Sentiment analysis.
For this example a tiny dataset of movie reviews is used in order to calssify the sentiment of each review (either positive or negative). The model used is retreived from [HuggingFace](https://huggingface.co) and uses the transformers library to create a pipeline that will process a previously processed text in order to assign it a label.
### Named Entity Recognition.
Named Entity Recogniton (NER) is an NLP application that extracts information from a text in order to localize and categorize predefined classes and labels like people, organizations, places, etc. 
![image](https://user-images.githubusercontent.com/58601226/200972256-6e47b2d9-7969-4c2a-85a8-ef93c2aff077.png)
To know more about NER you can follow this [link](https://en.wikipedia.org/wiki/Named-entity_recognition).

For this example a NER model is trained with a big dataset of tweets, then, after the model is trained a `resources` folder will be generated with many files htat contain the performance of the model and its results, the script uses this files to plot the loss of the training and dev datasets (that can also be interpreted as the loss in the testing) to appreciate better the performance of the model, the plot needs to be similar to the following (the only differences are that this plot takes more epochs to train and uses the complete NER datasets):

![Figure_1](https://user-images.githubusercontent.com/58601226/201160908-1df7a7ad-33a4-4558-8c5a-3a1fa00cf162.png)

This script uses two important `.env` variables: `REDUCED_NER_DATASET` and `NER_EPOCHS` that will be detailed in the [About environment variables](About environment variables) section.
### API translation evaluation.
This example focuses on the evaluation of two translation APIs using the [BLEU score](https://www.askpython.com/python/bleu-score) as evaluation metric. An english dataset is translated to spanish by the [Libre Translate API](https://github.com/LibreTranslate/LibreTranslate) and the [Deep Translate API](https://rapidapi.com/gatzuma/api/deep-translate1/) to finally be evaluated. Take into account that the Deep API has limited requests so be careful with how you use it. 
*Note:* The Libre Translate API is rather slow for the translation, please give it some time to finish its translation.

## About environment variables
The `.env` file has the following variables:
* `REDUCED_NER_DATASET` - a boolean variable that indicates if you want to use the reduced NER dataset (500 samples) or the original one (more than 45,000 samples) using the original dataset will make the training very slow.
* `NER_EPOCH` - a number that indicates how many epochs the model will train, a higher number will result in higher training times.
* `X_RAPID_API_KEY` - a string containing your API key for the Rapid API web page, to obtain one you need to register and subscribe to the Deep Translate API in the following [link](https://rapidapi.com/hub).
* `X_RAPID_API_HOST_DEEP` - a string containing the host for the Deep Translate API, this can be obtained checking the documentation after registering and subscribing in the Rapid API page.
Take into account that the `make_env.py` script *does not make the API key and API host for you* this variables need to be manually added to the `.env` file.

## Testing.
The [src](https://github.com/tamagochi-descompuesto/nlp-examples/tree/main/nlp/src) folder has testing files (noted by the testing_* prefix) these files contain scripts necessary to run the tests for most of the functionalities of the activities. 
You can run these test by using two different commands:
* **Running all the tests at once:** You can run all the testing files by first activating your anaconda or venv environment and locating within the [src](https://github.com/tamagochi-descompuesto/nlp-examples/tree/main/nlp/src) folder using the `cd` command (i.e. `cd nlp/src`). Finally you can run the two following commands:
  * `python -m unittest` to run all the test files without verbose output.
  * `python -m unittest -v` to run all the test files with verbose output.
* **Running one test at a time** To run only one test file at a time you need to follow the same steps shown in the running all the tests at once section and run one of the two following commands: 
  * `python -m unittest test_*.Testing` to run a single desired test file with verbose output, i.e., to run the [`test_ner.py`](nlp/src/test_ner.py) you will need to run `python -m unittest test_ner.Testing` from your command line or anaconda prompt.
  * `python -m unittest -v test_*.Testing` to run a single desired test file with verbose output, i.e., to run the [`test_ner.py`](nlp/src/test_ner.py) you will need to run `python -m unittest -v test_ner.Testing` from your command line or anaconda prompt.
